{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bddd045f",
   "metadata": {},
   "source": [
    "# BBM409 Assigment-3\n",
    "Students:\n",
    "Name Surname: Yusuf Koca\n",
    "Student-ID: 2200356013\n",
    "\n",
    "Name Surname: Mustafa Emir Peker\n",
    "Student-ID: 2200356011"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30dd5542",
   "metadata": {},
   "source": [
    "Here in this assignment we will try to classify articles with using naive bayes theorem and bag of words model. Bayes classifiers are a simple family of probabilistic classifier based on applying naive bayes theorem. Naive-Bayes assumes each feature is independent from the other. We will do our calculations based on this.\n",
    "\n",
    "In part1 we will guess which three words would be best to choose for the job.\n",
    "\n",
    "In part2 we will classify using unigram and bigram words and without using stopwords with using unigram and bigram words.\n",
    "\n",
    "In part3 we will calculate the top10 effective words and classify with using them. We will do the same with eliminating stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c7d8d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy.random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51e7bb90",
   "metadata": {},
   "outputs": [],
   "source": [
    "global df\n",
    "global numpy_df\n",
    "global list_of_banned_words\n",
    "list_of_banned_words = ['the', 'to', 'a', 'and', 'in', 'of', 's', 'i', 'for', 'he', 'of', 'in', 'is', 'that', 'on', 'it', 'was', 'are', 'or', 'has', 'been', 'not', 'an', 'but', 'have', 'with', 'at', 'his', 'we', 'be', 'will', 'as', 'from', 'said', 'its', 'by', '-', 'mr', 'would', 'will', 'they', 'had', 'their', 'this', 'who', 'us', 'which', 'more', 'also', 'were']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a0f2b6",
   "metadata": {},
   "source": [
    "Firtsly we read the data form .csv file. Then we shuffle it to get unbiased results. Lastly i splitted the data as %80 training and %20 test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "300c3ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('English Dataset.csv')\n",
    "# df to numpy array\n",
    "numpy_df = df.to_numpy()\n",
    "np.random.shuffle(numpy_df)\n",
    "training_data = numpy_df[0:1192]\n",
    "test_data = numpy_df[1192:1490]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4f411b",
   "metadata": {},
   "source": [
    "Here we have declared a class for each catagory. Each class has its dictionry that keeps sport related words' as keys and their occurunce numbers as values. Each class has a function for increasing the count of the word in that dictionary(class) , a function for getting the count of the given word, a function to getting the top 3 occured words and a function for getting the top 10 occuring words.\n",
    "\n",
    "\n",
    "For part1exe class first of we create an instance for each class we created for catagories. After that we loop through the data and we take the item indexed at 1. After getting that item we split that item to get words. After getting the words we also loop through the words and we put the word to appropiate class's dictionary. Important part is while putting the we check if that word is in the banned words list if the word is in the banned words list we don't put the word in to any dictionary. After that we call each catagories top 3 words function and we deterime the most informative words for each catagory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3426fdc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sport:\n",
    "    words_dict = {}\n",
    "\n",
    "    # increase the count of the word in the dictionary\n",
    "    def increase_count(self, word):\n",
    "        if word in self.words_dict:\n",
    "            self.words_dict[word] += 1\n",
    "        else:\n",
    "            self.words_dict[word] = 1\n",
    "\n",
    "    # get the count of the word in the dictionary\n",
    "    def get_count(self, word):\n",
    "        if word in self.words_dict:\n",
    "            return self.words_dict[word]\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    # get the top three occuring words and their counts\n",
    "    def get_top_three(self):\n",
    "        # if the top ten words are 'the, to, a, and, in, of, s, i, for, he, of, in, is, that, on, it, was' then they are ignored\n",
    "        count = 0\n",
    "        # loop through the dictionary\n",
    "        returning_dict = {}\n",
    "        for key, value in self.words_dict.items():\n",
    "            # if the word is not in the list of words to ignore\n",
    "            if key not in list_of_banned_words:\n",
    "                # add it to the returning dictionary\n",
    "                returning_dict[key] = value\n",
    "                count += 1\n",
    "            # if the count is 10 then break\n",
    "            if count == 3:\n",
    "                break\n",
    "\n",
    "        return sorted(returning_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "\n",
    "    def get_top_ten(self):\n",
    "        # if the top ten words are 'the, to, a, and, in, of, s, i, for, he, of, in, is, that, on, it, was' then they are ignored\n",
    "        # loop through the dictionary\n",
    "        returning_dict = {}\n",
    "        for key, value in self.words_dict.items():\n",
    "            # if the word is not in the list of words to ignore\n",
    "            if key not in list_of_banned_words:\n",
    "                # add it to the returning dictionary\n",
    "                returning_dict[key] = value\n",
    "\n",
    "\n",
    "        return sorted(returning_dict.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "\n",
    "class Business:\n",
    "    words_dict = {}\n",
    "\n",
    "    # increase the count of the word in the dictionary\n",
    "    def increase_count(self, word):\n",
    "        if word in self.words_dict:\n",
    "            self.words_dict[word] += 1\n",
    "        else:\n",
    "            self.words_dict[word] = 1\n",
    "\n",
    "    # get the count of the word in the dictionary\n",
    "    def get_count(self, word):\n",
    "        if word in self.words_dict:\n",
    "            return self.words_dict[word]\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    # get the top three occuring words and their counts\n",
    "    def get_top_three(self):\n",
    "        # if the top ten words are 'the, to, a, and, in, of, s, i, for, he, of, in, is, that, on, it, was' then they are ignored\n",
    "        count = 0\n",
    "        # loop through the dictionary\n",
    "        returning_dict = {}\n",
    "        for key, value in self.words_dict.items():\n",
    "            # if the word is not in the list of words to ignore\n",
    "            if key not in list_of_banned_words:\n",
    "                # add it to the returning dictionary\n",
    "                returning_dict[key] = value\n",
    "                count += 1\n",
    "            # if the count is 10 then break\n",
    "            if count == 3:\n",
    "                break\n",
    "\n",
    "        return sorted(returning_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    def get_top_ten(self):\n",
    "        # if the top ten words are 'the, to, a, and, in, of, s, i, for, he, of, in, is, that, on, it, was' then they are ignored\n",
    "        # loop through the dictionary\n",
    "        returning_dict = {}\n",
    "        for key, value in self.words_dict.items():\n",
    "            # if the word is not in the list of words to ignore\n",
    "            if key not in list_of_banned_words:\n",
    "                # add it to the returning dictionary\n",
    "                returning_dict[key] = value\n",
    "\n",
    "\n",
    "        return sorted(returning_dict.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "\n",
    "class Politics:\n",
    "\n",
    "    words_dict = {}\n",
    "\n",
    "    # increase the count of the word in the dictionary\n",
    "    def increase_count(self, word):\n",
    "        if word in self.words_dict:\n",
    "            self.words_dict[word] += 1\n",
    "        else:\n",
    "            self.words_dict[word] = 1\n",
    "\n",
    "    # get the count of the word in the dictionary\n",
    "    def get_count(self, word):\n",
    "        if word in self.words_dict:\n",
    "            return self.words_dict[word]\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    # get the top three occuring words and their counts\n",
    "    def get_top_three(self):\n",
    "        # if the top ten words are 'the, to, a, and, in, of, s, i, for, he, of, in, is, that, on, it, was' then they are ignored\n",
    "        count = 0\n",
    "        # loop through the dictionary\n",
    "        returning_dict = {}\n",
    "        for key, value in self.words_dict.items():\n",
    "            # if the word is not in the list of words to ignore\n",
    "            if key not in list_of_banned_words:\n",
    "                # add it to the returning dictionary\n",
    "                returning_dict[key] = value\n",
    "                count += 1\n",
    "            # if the count is 10 then break\n",
    "            if count == 3:\n",
    "                break\n",
    "\n",
    "        return sorted(returning_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    def get_top_ten(self):\n",
    "        # if the top ten words are 'the, to, a, and, in, of, s, i, for, he, of, in, is, that, on, it, was' then they are ignored\n",
    "        # loop through the dictionary\n",
    "        returning_dict = {}\n",
    "        for key, value in self.words_dict.items():\n",
    "            # if the word is not in the list of words to ignore\n",
    "            if key not in list_of_banned_words:\n",
    "                # add it to the returning dictionary\n",
    "                returning_dict[key] = value\n",
    "\n",
    "\n",
    "        return sorted(returning_dict.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "\n",
    "class Tech:\n",
    "    words_dict = {}\n",
    "\n",
    "    # increase the count of the word in the dictionary\n",
    "    def increase_count(self, word):\n",
    "        if word in self.words_dict:\n",
    "            self.words_dict[word] += 1\n",
    "        else:\n",
    "            self.words_dict[word] = 1\n",
    "\n",
    "    # get the count of the word in the dictionary\n",
    "    def get_count(self, word):\n",
    "        if word in self.words_dict:\n",
    "            return self.words_dict[word]\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    # get the top three occuring words and their counts\n",
    "    def get_top_three(self):\n",
    "        # if the top ten words are 'the, to, a, and, in, of, s, i, for, he, of, in, is, that, on, it, was' then they are ignored\n",
    "        count = 0\n",
    "        # loop through the dictionary\n",
    "        returning_dict = {}\n",
    "        for key, value in self.words_dict.items():\n",
    "            # if the word is not in the list of words to ignore\n",
    "            if key not in list_of_banned_words:\n",
    "                # add it to the returning dictionary\n",
    "                returning_dict[key] = value\n",
    "                count += 1\n",
    "            # if the count is 10 then break\n",
    "            if count == 3:\n",
    "                break\n",
    "\n",
    "        return sorted(returning_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    def get_top_ten(self):\n",
    "        # if the top ten words are 'the, to, a, and, in, of, s, i, for, he, of, in, is, that, on, it, was' then they are ignored\n",
    "        # loop through the dictionary\n",
    "        returning_dict = {}\n",
    "        for key, value in self.words_dict.items():\n",
    "            # if the word is not in the list of words to ignore\n",
    "            if key not in list_of_banned_words:\n",
    "                # add it to the returning dictionary\n",
    "                returning_dict[key] = value\n",
    "\n",
    "\n",
    "        return sorted(returning_dict.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "\n",
    "class Entertainment:\n",
    "    words_dict = {}\n",
    "\n",
    "    # increase the count of the word in the dictionary\n",
    "    def increase_count(self, word):\n",
    "        if word in self.words_dict:\n",
    "            self.words_dict[word] += 1\n",
    "        else:\n",
    "            self.words_dict[word] = 1\n",
    "\n",
    "    # get the count of the word in the dictionary\n",
    "    def get_count(self, word):\n",
    "        if word in self.words_dict:\n",
    "            return self.words_dict[word]\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    # get the top three occuring words and their counts\n",
    "    def get_top_three(self):\n",
    "        # if the top ten words are 'the, to, a, and, in, of, s, i, for, he, of, in, is, that, on, it, was' then they are ignored\n",
    "\n",
    "        # loop through the dictionary\n",
    "        returning_dict = {}\n",
    "        for key, value in self.words_dict.items():\n",
    "            # if the word is not in the list of words to ignore\n",
    "            if key not in list_of_banned_words:\n",
    "                # add it to the returning dictionary\n",
    "                returning_dict[key] = value\n",
    "\n",
    "\n",
    "        return sorted(returning_dict.items(), key=lambda x: x[1], reverse=True)[:3]\n",
    "\n",
    "    def get_top_ten(self):\n",
    "        # if the top ten words are 'the, to, a, and, in, of, s, i, for, he, of, in, is, that, on, it, was' then they are ignored\n",
    "        # loop through the dictionary\n",
    "        returning_dict = {}\n",
    "        for key, value in self.words_dict.items():\n",
    "            # if the word is not in the list of words to ignore\n",
    "            if key not in list_of_banned_words:\n",
    "                # add it to the returning dictionary\n",
    "                returning_dict[key] = value\n",
    "\n",
    "\n",
    "        return sorted(returning_dict.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "\n",
    "class part1_exe:\n",
    "    sport = Sport()\n",
    "    business = Business()\n",
    "    politics = Politics()\n",
    "    tech = Tech()\n",
    "    entertainment = Entertainment()\n",
    "\n",
    "    def count_words(self, numpy_array):\n",
    "        for row in numpy_array:\n",
    "            if row[2] == 'sport':\n",
    "                for word in row[1].split():\n",
    "                    self.sport.increase_count(word)\n",
    "            elif row[2] == 'business':\n",
    "                for word in row[1].split():\n",
    "                    self.business.increase_count(word)\n",
    "            elif row[2] == 'politics':\n",
    "                for word in row[1].split():\n",
    "                    self.politics.increase_count(word)\n",
    "            elif row[2] == 'tech':\n",
    "                for word in row[1].split():\n",
    "                    self.tech.increase_count(word)\n",
    "            elif row[2] == 'entertainment':\n",
    "                for word in row[1].split():\n",
    "                    self.entertainment.increase_count(word)\n",
    "\n",
    "    def get_top_three(self):\n",
    "        print('sport: ', self.sport.get_top_three())\n",
    "        print('business: ', self.business.get_top_three())\n",
    "        print('politics: ', self.politics.get_top_three())\n",
    "        print('tech: ', self.tech.get_top_three())\n",
    "        print('entertainment: ', self.entertainment.get_top_three())\n",
    "\n",
    "    def get_top_ten(self):\n",
    "        print('sport: ', self.sport.get_top_ten())\n",
    "        print('business: ', self.business.get_top_ten())\n",
    "        print('politics: ', self.politics.get_top_ten())\n",
    "        print('tech: ', self.tech.get_top_ten())\n",
    "        print('entertainment: ', self.entertainment.get_top_ten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7bb6bc49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def part1(numpy_df):\n",
    "    part1_obj = part1_exe()\n",
    "    part1_obj.count_words(numpy_df)\n",
    "    part1_obj.get_top_three()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc07a905",
   "metadata": {},
   "source": [
    "Here we have top 3 words of each type. We excluded the words like 'the , on, a , an ...'. While determining the exluded words we run our part1 until we had the actual key words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "abd6fef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sport:  [('beaten', 25), ('comeback', 16), ('henin-hardenne', 13)]\n",
      "business:  [('yukos', 109), ('rule', 19), ('refuge', 2)]\n",
      "politics:  [('straw', 59), ('ending', 5), ('backs', 3)]\n",
      "tech:  [('you', 344), ('how', 166), ('careful', 4)]\n",
      "entertainment:  [('film', 506), ('best', 404), ('one', 249)]\n"
     ]
    }
   ],
   "source": [
    "part1(numpy_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b064e4ba",
   "metadata": {},
   "source": [
    "Here we have four different classes to classify the test data and test our model. First classifies using unigram words, second classifies with bigram words, third classifies with unigram words but without using stop words and foruth classifies using bigram words but without using stopwords.\n",
    "\n",
    "Each class has five functions. amount_of_types function calculates how many elemens a class has in the training set. BagofWordsNgram functions creates a bag of words dictionary with key being the words and value being counts for the training set. vectorizerNgram function vectorizes the training set + current test data. frequenciesNgram gets the frequency of a word for each class that the test data has. nbNgram function finally handles the classification part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eb7b216d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this class handles naive bayes with unigram words\n",
    "class part2_unigram:\n",
    "    corpus = []\n",
    "    words_dict_unigram = {}\n",
    "    total_unigram = 0\n",
    "    unigram_guesses = []\n",
    "\n",
    "    total_sport = 0\n",
    "    total_business = 0\n",
    "    total_politics = 0\n",
    "    total_tech = 0\n",
    "    total_entertainment = 0\n",
    "\n",
    "    #gets how many items in the training in each class\n",
    "    def amount_of_types(self, np_Array):\n",
    "        for row in np_Array:\n",
    "            if row[2] == \"sport\":\n",
    "                self.total_sport = self.total_sport + 1\n",
    "            elif row[2] == \"business\":\n",
    "                self.total_business = self.total_business + 1\n",
    "            elif row[2] == \"politics\":\n",
    "                self.total_politics = self.total_politics + 1\n",
    "            elif row[2] == \"tech\":\n",
    "                self.total_tech = self.total_tech + 1\n",
    "            elif row[2] == \"entertainment\":\n",
    "                self.total_entertainment = self.total_entertainment + 1\n",
    "\n",
    "    #creates dictionary for bag of words with words name as a key and count of it as a value            \n",
    "    def BagofWordsUnigram(self, np_Array):\n",
    "        for row in np_Array:\n",
    "            for word in row[1].split():\n",
    "                if word in self.words_dict_unigram:\n",
    "                    self.words_dict_unigram[word] = self.words_dict_unigram[word] + 1\n",
    "                else:\n",
    "                    self.words_dict_unigram[word] = 1\n",
    "\n",
    "            self.corpus.append(row[1])\n",
    "\n",
    "        for key in self.words_dict_unigram:\n",
    "             self.total_unigram = self.words_dict_unigram[key] + self.total_unigram\n",
    "\n",
    "    #vectorizes the training set + the current test data           \n",
    "    def vectorizerUnigram(self, test, training):\n",
    "        vectorizer = CountVectorizer()\n",
    "        self.corpus.append(test[1])\n",
    "        X = vectorizer.fit_transform(self.corpus)\n",
    "        vectorizer.get_feature_names_out()\n",
    "        X = X.toarray()\n",
    "        self.corpus.pop()\n",
    "        self.frequenciesUnigram(X, training)\n",
    "\n",
    "    #gets frequency of a word that is in the test data for each class type    \n",
    "    def frequenciesUnigram(self, X, training):\n",
    "        index_list = []\n",
    "        frequency_list_sport = []\n",
    "        frequency_list_business = []\n",
    "        frequency_list_politics = []\n",
    "        frequency_list_tech = []\n",
    "        frequency_list_entertainment = []\n",
    "        for element in range(len(X[len(X)-1])):\n",
    "            if X[len(X)-1][element] > 0:\n",
    "                index_list.append(element)\n",
    "\n",
    "        for i in range(len(index_list)):\n",
    "            frequency_list_sport.append(0)\n",
    "            frequency_list_business.append(0)\n",
    "            frequency_list_politics.append(0)\n",
    "            frequency_list_tech.append(0)\n",
    "            frequency_list_entertainment.append(0)\n",
    "            for element in range(len(X)-1):\n",
    "                if training[element][2] == \"sport\":\n",
    "                    frequency_list_sport[i] = frequency_list_sport[i] + X[element][index_list[i]]\n",
    "                elif training[element][2] == \"business\":\n",
    "                    frequency_list_business[i] = frequency_list_business[i] + X[element][index_list[i]]\n",
    "                elif training[element][2] == \"politics\":\n",
    "                    frequency_list_politics[i] = frequency_list_politics[i] + X[element][index_list[i]]\n",
    "                elif training[element][2] == \"tech\":\n",
    "                    frequency_list_tech[i] = frequency_list_tech[i] + X[element][index_list[i]]\n",
    "                elif training[element][2] == \"entertainment\":\n",
    "                    frequency_list_entertainment[i] = frequency_list_entertainment[i] + X[element][index_list[i]]\n",
    "\n",
    "        self.nbUnigram(frequency_list_sport, frequency_list_business, frequency_list_politics, frequency_list_tech, frequency_list_entertainment, X[len(X)-1], training, index_list)\n",
    "\n",
    "    #handles the classification of the test data\n",
    "    def nbUnigram(self, fs, fb, fp, ft, fe, test, training, index_list):\n",
    "\n",
    "\n",
    "        nb_s = round(math.log(self.total_sport/len(training)), 8)\n",
    "        nb_b = round(math.log(self.total_business/len(training)), 8)\n",
    "        nb_p = round(math.log(self.total_politics/len(training)), 8)\n",
    "        nb_t = round(math.log(self.total_tech/len(training)), 8)\n",
    "        nb_e = round(math.log(self.total_entertainment/len(training)), 8)\n",
    "\n",
    "        #nb with add one smoothing\n",
    "        if (0 in fs) or (0 in fb) or (0 in fp) or (0 in ft) or (0 in fe):\n",
    "\n",
    "            for i in range(len(fs)):\n",
    "                fs[i] = fs[i] + 1\n",
    "                fb[i] = fb[i] + 1\n",
    "                fp[i] = fp[i] + 1\n",
    "                ft[i] = ft[i] + 1\n",
    "                fe[i] = fe[i] + 1\n",
    "\n",
    "            for i in range(len(fs)):\n",
    "                for j in range(test[index_list[i]]):\n",
    "                    nb_s = nb_s + (round((math.log(fs[i] / ((self.total_unigram) + len(self.words_dict_unigram)))), 8))\n",
    "                for j in range(test[index_list[i]]):\n",
    "                    nb_b = nb_b + (round((math.log(fb[i] / ((self.total_unigram) + len(self.words_dict_unigram)))), 8))\n",
    "                for j in range(test[index_list[i]]):\n",
    "                    nb_p = nb_p + (round((math.log(fp[i] / ((self.total_unigram) + len(self.words_dict_unigram)))), 8))\n",
    "                for j in range(test[index_list[i]]):\n",
    "                    nb_t = nb_t + (round((math.log(ft[i] / ((self.total_unigram) + len(self.words_dict_unigram)))), 8))\n",
    "                for j in range(test[index_list[i]]):\n",
    "                    nb_e = nb_e + (round((math.log(fe[i] / ((self.total_unigram) + len(self.words_dict_unigram)))), 8))\n",
    "        \n",
    "        #nb without smoothing\n",
    "        else:\n",
    "\n",
    "            for i in range(len(fs)):\n",
    "                for j in range(test[index_list[i]]):\n",
    "                    nb_s = nb_s + (round((math.log(fs[i] / (self.total_unigram))), 8))\n",
    "                for j in range(test[index_list[i]]):\n",
    "                    nb_b = nb_b + (round((math.log(fb[i] / (self.total_unigram))), 8))\n",
    "                for j in range(test[index_list[i]]):\n",
    "                    nb_p = nb_p + (round((math.log(fp[i] / (self.total_unigram))), 8))\n",
    "                for j in range(test[index_list[i]]):\n",
    "                    nb_t = nb_t + (round((math.log(ft[i] / (self.total_unigram))), 8))\n",
    "                for j in range(test[index_list[i]]):\n",
    "                    nb_e = nb_e + (round((math.log(fe[i] / (self.total_unigram))), 8))\n",
    "\n",
    "        results = [nb_s, nb_b, nb_p, nb_t, nb_e]\n",
    "\n",
    "        if max(results) == nb_s:\n",
    "            self.unigram_guesses.append(\"sport\")\n",
    "        elif max(results) == nb_b:\n",
    "            self.unigram_guesses.append(\"business\")\n",
    "        elif max(results) == nb_p:\n",
    "            self.unigram_guesses.append(\"politics\")\n",
    "        elif max(results) == nb_t:\n",
    "            self.unigram_guesses.append(\"tech\")\n",
    "        elif max(results) == nb_e:\n",
    "            self.unigram_guesses.append(\"entertainment\")\n",
    "\n",
    "#this class handles naive bayes with bigram words\n",
    "class part2_bigram:\n",
    "    corpus = []\n",
    "    words_dict_bigram = {}\n",
    "    total_bigram = 0\n",
    "    bigram_guesses = []\n",
    "\n",
    "    total_sport = 0\n",
    "    total_business = 0\n",
    "    total_politics = 0\n",
    "    total_tech = 0\n",
    "    total_entertainment = 0\n",
    "\n",
    "    #gets how many items in the training in each class\n",
    "    def amount_of_types(self, np_Array):\n",
    "        for row in np_Array:\n",
    "            if row[2] == \"sport\":\n",
    "                self.total_sport = self.total_sport + 1\n",
    "            elif row[2] == \"business\":\n",
    "                self.total_business = self.total_business + 1\n",
    "            elif row[2] == \"politics\":\n",
    "                self.total_politics = self.total_politics + 1\n",
    "            elif row[2] == \"tech\":\n",
    "                self.total_tech = self.total_tech + 1\n",
    "            elif row[2] == \"entertainment\":\n",
    "                self.total_entertainment = self.total_entertainment + 1\n",
    "\n",
    "    #creates dictionary for bag of words with words name as a key and count of it as a value            \n",
    "    def BagofWordsBigram(self, np_Array):\n",
    "        for row in np_Array:\n",
    "            wordlist = row[1].split(\" \")\n",
    "            for word in range(len(wordlist)-1):\n",
    "                if (wordlist[word] + \" \" + wordlist[word+1]) in self.words_dict_bigram:\n",
    "                    self.words_dict_bigram[wordlist[word] + \" \" + wordlist[word+1]] = self.words_dict_bigram[wordlist[word] + \" \" + wordlist[word+1]] + 1\n",
    "                else:\n",
    "                    self.words_dict_bigram[wordlist[word] + \" \" + wordlist[word+1]] = 1\n",
    "\n",
    "            self.corpus.append(row[1])\n",
    "\n",
    "        for key in self.words_dict_bigram:\n",
    "             self.total_bigram = self.words_dict_bigram[key] + self.total_bigram\n",
    "\n",
    "    #vectorizes the training set + the current test data            \n",
    "    def vectorizerBigram(self, test, training):\n",
    "        vectorizer = CountVectorizer(analyzer=\"word\", ngram_range=(2, 2))\n",
    "        self.corpus.append(test[1])\n",
    "        X = vectorizer.fit_transform(self.corpus)\n",
    "        vectorizer.get_feature_names_out()\n",
    "        X = X.toarray()\n",
    "        self.corpus.pop()\n",
    "        self.frequenciesBigram(X, training)\n",
    "\n",
    "    #gets frequency of a word that is in the test data for each class type    \n",
    "    def frequenciesBigram(self, X, training):\n",
    "        index_list = []\n",
    "        frequency_list_sport = []\n",
    "        frequency_list_business = []\n",
    "        frequency_list_politics = []\n",
    "        frequency_list_tech = []\n",
    "        frequency_list_entertainment = []\n",
    "        for element in range(len(X[len(X)-1])):\n",
    "            if X[len(X)-1][element] > 0:\n",
    "                index_list.append(element)\n",
    "\n",
    "        for i in range(len(index_list)):\n",
    "            frequency_list_sport.append(0)\n",
    "            frequency_list_business.append(0)\n",
    "            frequency_list_politics.append(0)\n",
    "            frequency_list_tech.append(0)\n",
    "            frequency_list_entertainment.append(0)\n",
    "            for element in range(len(X)-1):\n",
    "                if training[element][2] == \"sport\":\n",
    "                    frequency_list_sport[i] = frequency_list_sport[i] + X[element][index_list[i]]\n",
    "                elif training[element][2] == \"business\":\n",
    "                    frequency_list_business[i] = frequency_list_business[i] + X[element][index_list[i]]\n",
    "                elif training[element][2] == \"politics\":\n",
    "                    frequency_list_politics[i] = frequency_list_politics[i] + X[element][index_list[i]]\n",
    "                elif training[element][2] == \"tech\":\n",
    "                    frequency_list_tech[i] = frequency_list_tech[i] + X[element][index_list[i]]\n",
    "                elif training[element][2] == \"entertainment\":\n",
    "                    frequency_list_entertainment[i] = frequency_list_entertainment[i] + X[element][index_list[i]]\n",
    "\n",
    "        self.nbBigram(frequency_list_sport, frequency_list_business, frequency_list_politics, frequency_list_tech, frequency_list_entertainment, X[len(X)-1], training, index_list)\n",
    "\n",
    "    #handles the classification of the test data    \n",
    "    def nbBigram(self, fs, fb, fp, ft, fe, test, training, index_list):\n",
    "\n",
    "\n",
    "        nb_s = round(math.log(self.total_sport/len(training)), 8)\n",
    "        nb_b = round(math.log(self.total_business/len(training)), 8)\n",
    "        nb_p = round(math.log(self.total_politics/len(training)), 8)\n",
    "        nb_t = round(math.log(self.total_tech/len(training)), 8)\n",
    "        nb_e = round(math.log(self.total_entertainment/len(training)), 8)\n",
    "\n",
    "        #nb with add one smoothing\n",
    "        if (0 in fs) or (0 in fb) or (0 in fp) or (0 in ft) or (0 in fe):\n",
    "\n",
    "            for i in range(len(fs)):\n",
    "                fs[i] = fs[i] + 1\n",
    "                fb[i] = fb[i] + 1\n",
    "                fp[i] = fp[i] + 1\n",
    "                ft[i] = ft[i] + 1\n",
    "                fe[i] = fe[i] + 1\n",
    "\n",
    "            for i in range(len(fs)):\n",
    "                for j in range(test[index_list[i]]):\n",
    "                    nb_s = nb_s + (round((math.log(fs[i] / ((self.total_bigram) + len(self.words_dict_bigram)))), 8))\n",
    "                for j in range(test[index_list[i]]):\n",
    "                    nb_b = nb_b + (round((math.log(fb[i] / ((self.total_bigram) + len(self.words_dict_bigram)))), 8))\n",
    "                for j in range(test[index_list[i]]):\n",
    "                    nb_p = nb_p + (round((math.log(fp[i] / ((self.total_bigram) + len(self.words_dict_bigram)))), 8))\n",
    "                for j in range(test[index_list[i]]):\n",
    "                    nb_t = nb_t + (round((math.log(ft[i] / ((self.total_bigram) + len(self.words_dict_bigram)))), 8))\n",
    "                for j in range(test[index_list[i]]):\n",
    "                    nb_e = nb_e + (round((math.log(fe[i] / ((self.total_bigram) + len(self.words_dict_bigram)))), 8))\n",
    "        \n",
    "        #nb without smoothing\n",
    "        else:\n",
    "\n",
    "            for i in range(len(fs)):\n",
    "                for j in range(test[index_list[i]]):\n",
    "                    nb_s = nb_s + (round((math.log(fs[i] / (self.total_bigram))), 8))\n",
    "                for j in range(test[index_list[i]]):\n",
    "                    nb_b = nb_b + (round((math.log(fb[i] / (self.total_bigram))), 8))\n",
    "                for j in range(test[index_list[i]]):\n",
    "                    nb_p = nb_p + (round((math.log(fp[i] / (self.total_bigram))), 8))\n",
    "                for j in range(test[index_list[i]]):\n",
    "                    nb_t = nb_t + (round((math.log(ft[i] / (self.total_bigram))), 8))\n",
    "                for j in range(test[index_list[i]]):\n",
    "                    nb_e = nb_e + (round((math.log(fe[i] / (self.total_bigram))), 8))\n",
    "\n",
    "        results = [nb_s, nb_b, nb_p, nb_t, nb_e]\n",
    "\n",
    "        if max(results) == nb_s:\n",
    "            self.bigram_guesses.append(\"sport\")\n",
    "        elif max(results) == nb_b:\n",
    "            self.bigram_guesses.append(\"business\")\n",
    "        elif max(results) == nb_p:\n",
    "            self.bigram_guesses.append(\"politics\")\n",
    "        elif max(results) == nb_t:\n",
    "            self.bigram_guesses.append(\"tech\")\n",
    "        elif max(results) == nb_e:\n",
    "            self.bigram_guesses.append(\"entertainment\")\n",
    "\n",
    "#this class handles naive bayes with unigram words without using stopwords            \n",
    "class part2_stopwords_unigram:\n",
    "    corpus = []\n",
    "    words_dict_unigram = {}\n",
    "    total_unigram = 0\n",
    "    unigram_guesses = []\n",
    "\n",
    "    total_sport = 0\n",
    "    total_business = 0\n",
    "    total_politics = 0\n",
    "    total_tech = 0\n",
    "    total_entertainment = 0\n",
    "\n",
    "    #gets how many items in the training in each class\n",
    "    def amount_of_types(self, np_Array):\n",
    "        for row in np_Array:\n",
    "            if row[2] == \"sport\":\n",
    "                self.total_sport = self.total_sport + 1\n",
    "            elif row[2] == \"business\":\n",
    "                self.total_business = self.total_business + 1\n",
    "            elif row[2] == \"politics\":\n",
    "                self.total_politics = self.total_politics + 1\n",
    "            elif row[2] == \"tech\":\n",
    "                self.total_tech = self.total_tech + 1\n",
    "            elif row[2] == \"entertainment\":\n",
    "                self.total_entertainment = self.total_entertainment + 1\n",
    "\n",
    "    #creates dictionary for bag of words with words name as a key and count of it as a value \n",
    "    def BagofWordsUnigram(self, np_Array):\n",
    "        for row in np_Array:\n",
    "            for word in row[1]:\n",
    "                if word not in ENGLISH_STOP_WORDS:\n",
    "                    if word in self.words_dict_unigram:\n",
    "                        self.words_dict_unigram[word] = self.words_dict_unigram[word] + 1\n",
    "                    else:\n",
    "                        self.words_dict_unigram[word] = 1\n",
    "\n",
    "            self.corpus.append(row[1])\n",
    "\n",
    "        for key in self.words_dict_unigram:\n",
    "             self.total_unigram = self.words_dict_unigram[key] + self.total_unigram\n",
    "\n",
    "    #vectorizes the training set + the current test data\n",
    "    def vectorizerUnigram(self, test, training):\n",
    "        vectorizer = CountVectorizer(analyzer=\"word\", stop_words=ENGLISH_STOP_WORDS)\n",
    "        self.corpus.append(test[1])\n",
    "        X = vectorizer.fit_transform(self.corpus)\n",
    "        vectorizer.get_feature_names_out()\n",
    "        X = X.toarray()\n",
    "        self.corpus.pop()\n",
    "        self.frequenciesUnigram(X, training)\n",
    "\n",
    "    #gets frequency of a word that is in the test data for each class type\n",
    "    def frequenciesUnigram(self, X, training):\n",
    "        index_list = []\n",
    "        frequency_list_sport = []\n",
    "        frequency_list_business = []\n",
    "        frequency_list_politics = []\n",
    "        frequency_list_tech = []\n",
    "        frequency_list_entertainment = []\n",
    "        for element in range(len(X[len(X)-1])):\n",
    "            if X[len(X)-1][element] > 0:\n",
    "                index_list.append(element)\n",
    "\n",
    "        for i in range(len(index_list)):\n",
    "            frequency_list_sport.append(0)\n",
    "            frequency_list_business.append(0)\n",
    "            frequency_list_politics.append(0)\n",
    "            frequency_list_tech.append(0)\n",
    "            frequency_list_entertainment.append(0)\n",
    "            for element in range(len(X)-1):\n",
    "                if training[element][2] == \"sport\":\n",
    "                    frequency_list_sport[i] = frequency_list_sport[i] + X[element][index_list[i]]\n",
    "                elif training[element][2] == \"business\":\n",
    "                    frequency_list_business[i] = frequency_list_business[i] + X[element][index_list[i]]\n",
    "                elif training[element][2] == \"politics\":\n",
    "                    frequency_list_politics[i] = frequency_list_politics[i] + X[element][index_list[i]]\n",
    "                elif training[element][2] == \"tech\":\n",
    "                    frequency_list_tech[i] = frequency_list_tech[i] + X[element][index_list[i]]\n",
    "                elif training[element][2] == \"entertainment\":\n",
    "                    frequency_list_entertainment[i] = frequency_list_entertainment[i] + X[element][index_list[i]]\n",
    "\n",
    "        self.nbUnigram(frequency_list_sport, frequency_list_business, frequency_list_politics, frequency_list_tech, frequency_list_entertainment, X[len(X)-1], training, index_list)\n",
    "\n",
    "    #handles the classification of the test data\n",
    "    def nbUnigram(self, fs, fb, fp, ft, fe, test, training, index_list):\n",
    "\n",
    "        nb_s = round(math.log(self.total_sport / len(training)), 8)\n",
    "        nb_b = round(math.log(self.total_business / len(training)), 8)\n",
    "        nb_p = round(math.log(self.total_politics / len(training)), 8)\n",
    "        nb_t = round(math.log(self.total_tech / len(training)), 8)\n",
    "        nb_e = round(math.log(self.total_entertainment / len(training)), 8)\n",
    "\n",
    "        #nb with add one smoothing\n",
    "        if (0 in fs) or (0 in fb) or (0 in fp) or (0 in ft) or (0 in fe):\n",
    "\n",
    "            for i in range(len(fs)):\n",
    "                fs[i] = fs[i] + 1\n",
    "                fb[i] = fb[i] + 1\n",
    "                fp[i] = fp[i] + 1\n",
    "                ft[i] = ft[i] + 1\n",
    "                fe[i] = fe[i] + 1\n",
    "\n",
    "            for i in range(len(fs)):\n",
    "                for j in range(test[index_list[i]]):\n",
    "                    nb_s = nb_s + (round((math.log(fs[i] / ((self.total_unigram) + len(self.words_dict_unigram)))), 8))\n",
    "                for j in range(test[index_list[i]]):\n",
    "                    nb_b = nb_b + (round((math.log(fb[i] / ((self.total_unigram) + len(self.words_dict_unigram)))), 8))\n",
    "                for j in range(test[index_list[i]]):\n",
    "                    nb_p = nb_p + (round((math.log(fp[i] / ((self.total_unigram) + len(self.words_dict_unigram)))), 8))\n",
    "                for j in range(test[index_list[i]]):\n",
    "                    nb_t = nb_t + (round((math.log(ft[i] / ((self.total_unigram) + len(self.words_dict_unigram)))), 8))\n",
    "                for j in range(test[index_list[i]]):\n",
    "                    nb_e = nb_e + (round((math.log(fe[i] / ((self.total_unigram) + len(self.words_dict_unigram)))), 8))\n",
    "        \n",
    "        #nb without smoothing\n",
    "        else:\n",
    "\n",
    "            for i in range(len(fs)):\n",
    "                for j in range(test[index_list[i]]):\n",
    "                    nb_s = nb_s + (round((math.log(fs[i] / (self.total_unigram))), 8))\n",
    "                for j in range(test[index_list[i]]):\n",
    "                    nb_b = nb_b + (round((math.log(fb[i] / (self.total_unigram))), 8))\n",
    "                for j in range(test[index_list[i]]):\n",
    "                    nb_p = nb_p + (round((math.log(fp[i] / (self.total_unigram))), 8))\n",
    "                for j in range(test[index_list[i]]):\n",
    "                    nb_t = nb_t + (round((math.log(ft[i] / (self.total_unigram))), 8))\n",
    "                for j in range(test[index_list[i]]):\n",
    "                    nb_e = nb_e + (round((math.log(fe[i] / (self.total_unigram))), 8))\n",
    "\n",
    "        results = [nb_s, nb_b, nb_p, nb_t, nb_e]\n",
    "\n",
    "        if max(results) == nb_s:\n",
    "            self.unigram_guesses.append(\"sport\")\n",
    "        elif max(results) == nb_b:\n",
    "            self.unigram_guesses.append(\"business\")\n",
    "        elif max(results) == nb_p:\n",
    "            self.unigram_guesses.append(\"politics\")\n",
    "        elif max(results) == nb_t:\n",
    "            self.unigram_guesses.append(\"tech\")\n",
    "        elif max(results) == nb_e:\n",
    "            self.unigram_guesses.append(\"entertainment\")\n",
    "\n",
    "#this class handles naive bayes with bigram words without using stopwords             \n",
    "class part2_stopwords_bigram:\n",
    "    corpus = []\n",
    "    words_dict_bigram = {}\n",
    "    total_bigram = 0\n",
    "    bigram_guesses = []\n",
    "\n",
    "    total_sport = 0\n",
    "    total_business = 0\n",
    "    total_politics = 0\n",
    "    total_tech = 0\n",
    "    total_entertainment = 0\n",
    "\n",
    "    #gets how many items in the training in each class\n",
    "    def amount_of_types(self, np_Array):\n",
    "        for row in np_Array:\n",
    "            if row[2] == \"sport\":\n",
    "                self.total_sport = self.total_sport + 1\n",
    "            elif row[2] == \"business\":\n",
    "                self.total_business = self.total_business + 1\n",
    "            elif row[2] == \"politics\":\n",
    "                self.total_politics = self.total_politics + 1\n",
    "            elif row[2] == \"tech\":\n",
    "                self.total_tech = self.total_tech + 1\n",
    "            elif row[2] == \"entertainment\":\n",
    "                self.total_entertainment = self.total_entertainment + 1\n",
    "\n",
    "    #creates dictionary for bag of words with words name as a key and count of it as a value \n",
    "    def BagofWordsBigram(self, np_Array):\n",
    "        for row in np_Array:\n",
    "            wordlist = row[1].split(\" \")\n",
    "            for word in range(len(wordlist)-1):\n",
    "                if (wordlist[word] not in ENGLISH_STOP_WORDS) and (wordlist[word+1] not in ENGLISH_STOP_WORDS):\n",
    "                    if (wordlist[word] + \" \" + wordlist[word+1]) in self.words_dict_bigram:\n",
    "                        self.words_dict_bigram[wordlist[word] + \" \" + wordlist[word+1]] = self.words_dict_bigram[wordlist[word] + \" \" + wordlist[word+1]] + 1\n",
    "                    else:\n",
    "                        self.words_dict_bigram[wordlist[word] + \" \" + wordlist[word+1]] = 1\n",
    "\n",
    "            self.corpus.append(row[1])\n",
    "\n",
    "        for key in self.words_dict_bigram:\n",
    "             self.total_bigram = self.words_dict_bigram[key] + self.total_bigram\n",
    "\n",
    "    #vectorizes the training set + the current test data\n",
    "    def vectorizerBigram(self, test, training):\n",
    "        vectorizer = CountVectorizer(analyzer=\"word\", ngram_range=(2, 2), stop_words=ENGLISH_STOP_WORDS)\n",
    "        self.corpus.append(test[1])\n",
    "        X = vectorizer.fit_transform(self.corpus)\n",
    "        vectorizer.get_feature_names_out()\n",
    "        X = X.toarray()\n",
    "        self.corpus.pop()\n",
    "        self.frequenciesBigram(X, training)\n",
    "\n",
    "    #gets frequency of a word that is in the test data for each class type\n",
    "    def frequenciesBigram(self, X, training):\n",
    "        index_list = []\n",
    "        frequency_list_sport = []\n",
    "        frequency_list_business = []\n",
    "        frequency_list_politics = []\n",
    "        frequency_list_tech = []\n",
    "        frequency_list_entertainment = []\n",
    "        for element in range(len(X[len(X)-1])):\n",
    "            if X[len(X)-1][element] > 0:\n",
    "                index_list.append(element)\n",
    "\n",
    "        for i in range(len(index_list)):\n",
    "            frequency_list_sport.append(0)\n",
    "            frequency_list_business.append(0)\n",
    "            frequency_list_politics.append(0)\n",
    "            frequency_list_tech.append(0)\n",
    "            frequency_list_entertainment.append(0)\n",
    "            for element in range(len(X)-1):\n",
    "                if training[element][2] == \"sport\":\n",
    "                    frequency_list_sport[i] = frequency_list_sport[i] + X[element][index_list[i]]\n",
    "                elif training[element][2] == \"business\":\n",
    "                    frequency_list_business[i] = frequency_list_business[i] + X[element][index_list[i]]\n",
    "                elif training[element][2] == \"politics\":\n",
    "                    frequency_list_politics[i] = frequency_list_politics[i] + X[element][index_list[i]]\n",
    "                elif training[element][2] == \"tech\":\n",
    "                    frequency_list_tech[i] = frequency_list_tech[i] + X[element][index_list[i]]\n",
    "                elif training[element][2] == \"entertainment\":\n",
    "                    frequency_list_entertainment[i] = frequency_list_entertainment[i] + X[element][index_list[i]]\n",
    "\n",
    "        self.nbBigram(frequency_list_sport, frequency_list_business, frequency_list_politics, frequency_list_tech, frequency_list_entertainment, X[len(X)-1], training, index_list)\n",
    "\n",
    "    #handles the classification of the test data\n",
    "    def nbBigram(self, fs, fb, fp, ft, fe, test, training, index_list):\n",
    "\n",
    "\n",
    "        nb_s = round(math.log(self.total_sport/len(training)), 8)\n",
    "        nb_b = round(math.log(self.total_business/len(training)), 8)\n",
    "        nb_p = round(math.log(self.total_politics/len(training)), 8)\n",
    "        nb_t = round(math.log(self.total_tech/len(training)), 8)\n",
    "        nb_e = round(math.log(self.total_entertainment/len(training)), 8)\n",
    "\n",
    "        #nb with add one smoothing\n",
    "        if (0 in fs) or (0 in fb) or (0 in fp) or (0 in ft) or (0 in fe):\n",
    "\n",
    "            for i in range(len(fs)):\n",
    "                fs[i] = fs[i] + 1\n",
    "                fb[i] = fb[i] + 1\n",
    "                fp[i] = fp[i] + 1\n",
    "                ft[i] = ft[i] + 1\n",
    "                fe[i] = fe[i] + 1\n",
    "\n",
    "            for i in range(len(fs)):\n",
    "                for j in range(test[index_list[i]]):\n",
    "                    nb_s = nb_s + (round((math.log(fs[i] / ((self.total_bigram) + len(self.words_dict_bigram)))), 8))\n",
    "                for j in range(test[index_list[i]]):\n",
    "                    nb_b = nb_b + (round((math.log(fb[i] / ((self.total_bigram) + len(self.words_dict_bigram)))), 8))\n",
    "                for j in range(test[index_list[i]]):\n",
    "                    nb_p = nb_p + (round((math.log(fp[i] / ((self.total_bigram) + len(self.words_dict_bigram)))), 8))\n",
    "                for j in range(test[index_list[i]]):\n",
    "                    nb_t = nb_t + (round((math.log(ft[i] / ((self.total_bigram) + len(self.words_dict_bigram)))), 8))\n",
    "                for j in range(test[index_list[i]]):\n",
    "                    nb_e = nb_e + (round((math.log(fe[i] / ((self.total_bigram) + len(self.words_dict_bigram)))), 8))\n",
    "        \n",
    "        #nb without smoothing\n",
    "        else:\n",
    "\n",
    "            for i in range(len(fs)):\n",
    "                for j in range(test[index_list[i]]):\n",
    "                    nb_s = nb_s + (round((math.log(fs[i] / (self.total_bigram))), 8))\n",
    "                for j in range(test[index_list[i]]):\n",
    "                    nb_b = nb_b + (round((math.log(fb[i] / (self.total_bigram))), 8))\n",
    "                for j in range(test[index_list[i]]):\n",
    "                    nb_p = nb_p + (round((math.log(fp[i] / (self.total_bigram))), 8))\n",
    "                for j in range(test[index_list[i]]):\n",
    "                    nb_t = nb_t + (round((math.log(ft[i] / (self.total_bigram))), 8))\n",
    "                for j in range(test[index_list[i]]):\n",
    "                    nb_e = nb_e + (round((math.log(fe[i] / (self.total_bigram))), 8))\n",
    "\n",
    "        results = [nb_s, nb_b, nb_p, nb_t, nb_e]\n",
    "\n",
    "        if max(results) == nb_s:\n",
    "            self.bigram_guesses.append(\"sport\")\n",
    "        elif max(results) == nb_b:\n",
    "            self.bigram_guesses.append(\"business\")\n",
    "        elif max(results) == nb_p:\n",
    "            self.bigram_guesses.append(\"politics\")\n",
    "        elif max(results) == nb_t:\n",
    "            self.bigram_guesses.append(\"tech\")\n",
    "        elif max(results) == nb_e:\n",
    "            self.bigram_guesses.append(\"entertainment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c444248",
   "metadata": {},
   "source": [
    "Here the part2 function handles the part2 of our assignment. It creates an object from the above classes, classifies the test data then calculates the accuracy for each 4 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ee4869e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#handless all part2 operations\n",
    "def part2(training_data, test_data):\n",
    "\n",
    "    #classifying with unigram words\n",
    "    part2_unigram_obj = part2_unigram()\n",
    "    part2_unigram_obj.amount_of_types(training_data)\n",
    "    part2_unigram_obj.BagofWordsUnigram(training_data)\n",
    "    for test in test_data:\n",
    "        part2_unigram_obj.vectorizerUnigram(test, training_data)\n",
    "\n",
    "    count = 0\n",
    "\n",
    "    #getting accuracy for unigram words\n",
    "    for i in range(len(test_data)):\n",
    "        if test_data[i][2] == part2_unigram_obj.unigram_guesses[i]:\n",
    "            count = count + 1\n",
    "        else:\n",
    "            pass\n",
    "    print(\"Accuracy for Unigram guesses: \" + str(count/298))\n",
    "\n",
    "\n",
    "    #classifying with bigram words\n",
    "    part2_bigram_obj = part2_bigram()\n",
    "    part2_bigram_obj.amount_of_types(training_data)\n",
    "    part2_bigram_obj.BagofWordsBigram(training_data)\n",
    "    for test in test_data:\n",
    "        part2_bigram_obj.vectorizerBigram(test, training_data)\n",
    "\n",
    "    count2 = 0\n",
    "\n",
    "    #getting accuracy for bigram words\n",
    "    for i in range(len(test_data)):\n",
    "        if test_data[i][2] == part2_bigram_obj.bigram_guesses[i]:\n",
    "            count2 = count2 + 1\n",
    "        else:\n",
    "            pass\n",
    "    print(\"Accuracy for Bigram guesses: \" + str(count2/298))\n",
    "\n",
    "\n",
    "    #classifying with unigram words with stopwords\n",
    "    part2_stopwords_unigram_obj = part2_stopwords_unigram()\n",
    "    part2_stopwords_unigram_obj.amount_of_types(training_data)\n",
    "    part2_stopwords_unigram_obj.BagofWordsUnigram(training_data)\n",
    "    for test in test_data:\n",
    "        part2_stopwords_unigram_obj.vectorizerUnigram(test, training_data)\n",
    "\n",
    "    count3 = 0\n",
    "\n",
    "    #getting accuracy for unigram words with stopwords\n",
    "    for i in range(len(test_data)):\n",
    "        if test_data[i][2] == part2_stopwords_unigram_obj.unigram_guesses[i]:\n",
    "            count3 = count3 + 1\n",
    "        else:\n",
    "            pass\n",
    "    print(\"Accuracy for Unigram guesses with stop words: \" + str(count3/298))\n",
    "\n",
    "    \n",
    "    #classifying with bigram words with stopwords\n",
    "    part2_stopwords_bigram_obj = part2_stopwords_bigram()\n",
    "    part2_stopwords_bigram_obj.amount_of_types(training_data)\n",
    "    part2_stopwords_bigram_obj.BagofWordsBigram(training_data)\n",
    "    for test in test_data:\n",
    "        part2_stopwords_bigram_obj.vectorizerBigram(test, training_data)\n",
    "\n",
    "    count4 = 0\n",
    "\n",
    "    #getting accuracy for bigram words with stopwords\n",
    "    for i in range(len(test_data)):\n",
    "        if test_data[i][2] == part2_stopwords_bigram_obj.bigram_guesses[i]:\n",
    "            count4 = count4 + 1\n",
    "        else:\n",
    "            pass\n",
    "    print(\"Accuracy for Bigram guesses with stop words: \" + str(count4/298))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e410da",
   "metadata": {},
   "source": [
    "Here we get accuracies for each 4 different classifier above. When we multiply them by 100 we get %92, %94, %97, %98 respectively. From these results we can say while the bigram words gets better results than unigram words, getting rid of stopwords has a bigger impact. Reason for bigram words getting better results than unigram words can be a similar case to stopwords, as bigram words are less repetitive than unigram ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "920db1a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for Unigram guesses: 0.9228187919463087\n",
      "Accuracy for Bigram guesses: 0.9362416107382551\n",
      "Accuracy for Unigram guesses with stop words: 0.9731543624161074\n",
      "Accuracy for Bigram guesses with stop words: 0.9765100671140939\n"
     ]
    }
   ],
   "source": [
    "part2(training_data, test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffba8f9e",
   "metadata": {},
   "source": [
    "Here we have 2 top10 functions and 2 classifier classes for part3. top10 classes gets top10 effective words with using tf-idf algorith. Classifier classes actually works similar to part2 classifier except only using the results we get from top10 classes to create bag of words.\n",
    "\n",
    "top10 classes have 4 functions. amount_of_types function calculates how many elemens a class has in the training set. BagofWords functions creates a bag of words dictionary with key being the words and value being counts for the training set but it adds both unigram and bigram words to the bag. tfid functionhandles the tfid algorithm. getTenBestWords gets the most effective words from the vector tfid function created.\n",
    "\n",
    "Each classifier classes has five functions. amount_of_types function calculates how many elemens a class has in the training set. BagofWords functions creates a bag of words dictionary with most effective words with key being the words and value being counts for the training set. vectorizer function vectorizes the training set + current test data. frequencies gets the frequency of a word for each class that the test data has. nb function finally handles the classification part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a565be9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#gets top 10 best words for each class\n",
    "class part3_top10:\n",
    "    keys = []\n",
    "    words_dict = {}\n",
    "    words_count = {}\n",
    "    total = 0\n",
    "\n",
    "    total_sport = 0\n",
    "    total_business = 0\n",
    "    total_politics = 0\n",
    "    total_tech = 0\n",
    "    total_entertainment = 0\n",
    "\n",
    "    sports = \"\"\n",
    "    businesses = \"\"\n",
    "    politics = \"\"\n",
    "    techs = \"\"\n",
    "    entertainments = \"\"\n",
    "\n",
    "    best_sports = []\n",
    "    best_business = []\n",
    "    best_poltiics= []\n",
    "    best_tech = []\n",
    "    best_entertainment = []\n",
    "\n",
    "    #gets the amount of classes in the training set\n",
    "    def amount_of_types(self, np_Array):\n",
    "        for row in np_Array:\n",
    "            if row[2] == \"sport\":\n",
    "                self.total_sport = self.total_sport + 1\n",
    "                self.sports = self.sports + \" \" + row[1]\n",
    "            elif row[2] == \"business\":\n",
    "                self.total_business = self.total_business + 1\n",
    "                self.businesses = self.businesses + \" \" + row[1]\n",
    "            elif row[2] == \"politics\":\n",
    "                self.total_politics = self.total_politics + 1\n",
    "                self.politics = self.politics + \" \" + row[1]\n",
    "            elif row[2] == \"tech\":\n",
    "                self.total_tech = self.total_tech + 1\n",
    "                self.techs = self.techs + \" \" + row[1]\n",
    "            elif row[2] == \"entertainment\":\n",
    "                self.total_entertainment = self.total_entertainment + 1\n",
    "                self.entertainments = self.entertainments + \" \" + row[1]\n",
    "\n",
    "    #creates a bog of words dictionary including both unigram and bigram words            \n",
    "    def BagofWords(self, np_Array):\n",
    "        for row in np_Array:\n",
    "            for word in row[1].split():\n",
    "                if word in self.words_count:\n",
    "                    self.words_count[word] = self.words_count[word] + 1\n",
    "                else:\n",
    "                    self.words_count[word] = 1\n",
    "                    self.words_dict[word] = word\n",
    "\n",
    "        for row in np_Array:\n",
    "            wordlist = row[1].split(\" \")\n",
    "            for word in range(len(wordlist)-1):\n",
    "                if (wordlist[word] + \" \" + wordlist[word+1]) in self.words_count:\n",
    "                    self.words_count[wordlist[word] + \" \" + wordlist[word+1]] = self.words_count[wordlist[word] + \" \" + wordlist[word+1]] + 1\n",
    "                else:\n",
    "                    self.words_count[wordlist[word] + \" \" + wordlist[word+1]] = 1\n",
    "                    self.words_dict[wordlist[word] + \" \" + wordlist[word + 1]] = wordlist[word] + \" \" + wordlist[word + 1]\n",
    "\n",
    "        for key in self.words_dict:\n",
    "             self.total = self.words_count[key] + self.total\n",
    "             self.keys.append(key)\n",
    "\n",
    "        self.keys = sorted(self.keys)\n",
    "\n",
    "    #tfidf algorithm to get best words    \n",
    "    def tfid(self, training):\n",
    "        corpus = []\n",
    "        corpus.append(self.sports)\n",
    "        corpus.append(self.businesses)\n",
    "        corpus.append(self.politics)\n",
    "        corpus.append(self.techs)\n",
    "        corpus.append(self.entertainments)\n",
    "\n",
    "        cv = CountVectorizer(vocabulary=self.keys)\n",
    "        count_vector = cv.fit_transform(corpus)\n",
    "\n",
    "        tfidf_transformer = TfidfTransformer(smooth_idf=True, use_idf=True)\n",
    "        tfidf_transformer.fit(count_vector)\n",
    "\n",
    "        count_vector = cv.transform(corpus)\n",
    "        tf_idf_vector = tfidf_transformer.transform(count_vector)\n",
    "\n",
    "        self.getTenBestWords(tf_idf_vector.toarray())\n",
    "\n",
    "    #gets ten best words\n",
    "    def getTenBestWords(self, tf_idf):\n",
    "\n",
    "        sport = np.argpartition(tf_idf[0], -10)[-10:]\n",
    "        business = np.argpartition(tf_idf[1], -10)[-10:]\n",
    "        politics = np.argpartition(tf_idf[2], -10)[-10:]\n",
    "        tech = np.argpartition(tf_idf[3], -10)[-10:]\n",
    "        entertainment = np.argpartition(tf_idf[4], -10)[-10:]\n",
    "\n",
    "        for i in range(10):\n",
    "            self.best_sports.append(self.words_dict[self.keys[sport[i]]])\n",
    "            self.best_business.append(self.words_dict[self.keys[business[i]]])\n",
    "            self.best_poltiics.append(self.words_dict[self.keys[politics[i]]])\n",
    "            self.best_tech.append(self.words_dict[self.keys[tech[i]]])\n",
    "            self.best_entertainment.append(self.words_dict[self.keys[entertainment[i]]])\n",
    "\n",
    "\n",
    "        print(\"Effective words for sport are: \" + str(self.best_sports))\n",
    "        print(\"Effective words for business are: \" + str(self.best_business))\n",
    "        print(\"Effective words for politics are: \" + str(self.best_poltiics))\n",
    "        print(\"Effective words for tech are: \" + str(self.best_tech))\n",
    "        print(\"Effective words for entertainment are: \" + str(self.best_entertainment))\n",
    "\n",
    "#nb classifyer with only using top 10 words\n",
    "class part3_classifyer:\n",
    "    corpus = []\n",
    "    keys = []\n",
    "    words_dict = {}\n",
    "    total = 0\n",
    "    guesses = []\n",
    "\n",
    "    total_sport = 0\n",
    "    total_business = 0\n",
    "    total_politics = 0\n",
    "    total_tech = 0\n",
    "    total_entertainment = 0\n",
    "\n",
    "    #gets amount of classes in training set\n",
    "    def amount_of_types(self, np_Array):\n",
    "        for row in np_Array:\n",
    "            if row[2] == \"sport\":\n",
    "                self.total_sport = self.total_sport + 1\n",
    "            elif row[2] == \"business\":\n",
    "                self.total_business = self.total_business + 1\n",
    "            elif row[2] == \"politics\":\n",
    "                self.total_politics = self.total_politics + 1\n",
    "            elif row[2] == \"tech\":\n",
    "                self.total_tech = self.total_tech + 1\n",
    "            elif row[2] == \"entertainment\":\n",
    "                self.total_entertainment = self.total_entertainment + 1\n",
    "\n",
    "    #creates a bag of words dictionary with only top 10 words\n",
    "    def BagofWords(self, np_Array, top10):\n",
    "\n",
    "        for i in top10:\n",
    "            if i not in self.words_dict:\n",
    "                self.words_dict[i] = 0\n",
    "\n",
    "        for row in np_Array:\n",
    "            for word in row[1].split():\n",
    "                if word in self.words_dict:\n",
    "                    self.words_dict[word] = self.words_dict[word] + 1\n",
    "\n",
    "            self.corpus.append(row[1])\n",
    "\n",
    "        for key in self.words_dict:\n",
    "            self.total = self.words_dict[key] + self.total\n",
    "            self.keys.append(key)\n",
    "\n",
    "    #vectorizes the training set and test data\n",
    "    def vectorizer(self, test, training):\n",
    "        vectorizer = CountVectorizer(vocabulary=self.keys)\n",
    "        self.corpus.append(test[1])\n",
    "        X = vectorizer.fit_transform(self.corpus)\n",
    "        vectorizer.get_feature_names_out()\n",
    "        X = X.toarray()\n",
    "\n",
    "        self.corpus.pop()\n",
    "        self.frequencies(X, training)\n",
    "\n",
    "    #gets the frequencies of words for each class that the test data has\n",
    "    def frequencies(self, X, training):\n",
    "        index_list = []\n",
    "        frequency_list_sport = []\n",
    "        frequency_list_business = []\n",
    "        frequency_list_politics = []\n",
    "        frequency_list_tech = []\n",
    "        frequency_list_entertainment = []\n",
    "        for element in range(len(X[len(X) - 1])):\n",
    "            if X[len(X) - 1][element] > 0:\n",
    "                index_list.append(element)\n",
    "\n",
    "        for i in range(len(index_list)):\n",
    "            frequency_list_sport.append(0)\n",
    "            frequency_list_business.append(0)\n",
    "            frequency_list_politics.append(0)\n",
    "            frequency_list_tech.append(0)\n",
    "            frequency_list_entertainment.append(0)\n",
    "            for element in range(len(X) - 1):\n",
    "                if training[element][2] == \"sport\":\n",
    "                    frequency_list_sport[i] = frequency_list_sport[i] + X[element][index_list[i]]\n",
    "                elif training[element][2] == \"business\":\n",
    "                    frequency_list_business[i] = frequency_list_business[i] + X[element][index_list[i]]\n",
    "                elif training[element][2] == \"politics\":\n",
    "                    frequency_list_politics[i] = frequency_list_politics[i] + X[element][index_list[i]]\n",
    "                elif training[element][2] == \"tech\":\n",
    "                    frequency_list_tech[i] = frequency_list_tech[i] + X[element][index_list[i]]\n",
    "                elif training[element][2] == \"entertainment\":\n",
    "                    frequency_list_entertainment[i] = frequency_list_entertainment[i] + X[element][index_list[i]]\n",
    "\n",
    "        self.nb(frequency_list_sport, frequency_list_business, frequency_list_politics, frequency_list_tech,\n",
    "                       frequency_list_entertainment, X[len(X) - 1], training, index_list)\n",
    "\n",
    "    #nb classifyer\n",
    "    def nb(self, fs, fb, fp, ft, fe, test, training, index_list):\n",
    "\n",
    "        nb_s = round(math.log(self.total_sport / len(training)), 8)\n",
    "        nb_b = round(math.log(self.total_business / len(training)), 8)\n",
    "        nb_p = round(math.log(self.total_politics / len(training)), 8)\n",
    "        nb_t = round(math.log(self.total_tech / len(training)), 8)\n",
    "        nb_e = round(math.log(self.total_entertainment / len(training)), 8)\n",
    "\n",
    "        #nb with add one smoothing\n",
    "        if (0 in fs) or (0 in fb) or (0 in fp) or (0 in ft) or (0 in fe):\n",
    "\n",
    "            for i in range(len(fs)):\n",
    "                fs[i] = fs[i] + 1\n",
    "                fb[i] = fb[i] + 1\n",
    "                fp[i] = fp[i] + 1\n",
    "                ft[i] = ft[i] + 1\n",
    "                fe[i] = fe[i] + 1\n",
    "\n",
    "            for i in range(len(fs)):\n",
    "                for j in range(test[index_list[i]]):\n",
    "                    nb_s = nb_s + (round((math.log(fs[i] / ((self.total) + len(self.words_dict)))), 8))\n",
    "                for j in range(test[index_list[i]]):\n",
    "                    nb_b = nb_b + (round((math.log(fb[i] / ((self.total) + len(self.words_dict)))), 8))\n",
    "                for j in range(test[index_list[i]]):\n",
    "                    nb_p = nb_p + (round((math.log(fp[i] / ((self.total) + len(self.words_dict)))), 8))\n",
    "                for j in range(test[index_list[i]]):\n",
    "                    nb_t = nb_t + (round((math.log(ft[i] / ((self.total) + len(self.words_dict)))), 8))\n",
    "                for j in range(test[index_list[i]]):\n",
    "                    nb_e = nb_e + (round((math.log(fe[i] / ((self.total) + len(self.words_dict)))), 8))\n",
    "        \n",
    "        #nb without smoothing\n",
    "        else:\n",
    "\n",
    "            for i in range(len(fs)):\n",
    "                for j in range(test[index_list[i]]):\n",
    "                    nb_s = nb_s + (round((math.log(fs[i] / (self.total))), 8))\n",
    "                for j in range(test[index_list[i]]):\n",
    "                    nb_b = nb_b + (round((math.log(fb[i] / (self.total))), 8))\n",
    "                for j in range(test[index_list[i]]):\n",
    "                    nb_p = nb_p + (round((math.log(fp[i] / (self.total))), 8))\n",
    "                for j in range(test[index_list[i]]):\n",
    "                    nb_t = nb_t + (round((math.log(ft[i] / (self.total))), 8))\n",
    "                for j in range(test[index_list[i]]):\n",
    "                    nb_e = nb_e + (round((math.log(fe[i] / (self.total))), 8))\n",
    "\n",
    "        results = [nb_s, nb_b, nb_p, nb_t, nb_e]\n",
    "\n",
    "        if max(results) == nb_s:\n",
    "            self.guesses.append(\"sport\")\n",
    "        elif max(results) == nb_b:\n",
    "            self.guesses.append(\"business\")\n",
    "        elif max(results) == nb_p:\n",
    "            self.guesses.append(\"politics\")\n",
    "        elif max(results) == nb_t:\n",
    "            self.guesses.append(\"tech\")\n",
    "        elif max(results) == nb_e:\n",
    "            self.guesses.append(\"entertainment\")\n",
    "\n",
    "#gets top 10 best words for each class without stopwords            \n",
    "class part3_stopwords:\n",
    "    keys = []\n",
    "    words_dict = {}\n",
    "    words_count = {}\n",
    "    total = 0\n",
    "\n",
    "    total_sport = 0\n",
    "    total_business = 0\n",
    "    total_politics = 0\n",
    "    total_tech = 0\n",
    "    total_entertainment = 0\n",
    "\n",
    "    sports = \"\"\n",
    "    businesses = \"\"\n",
    "    politics = \"\"\n",
    "    techs = \"\"\n",
    "    entertainments = \"\"\n",
    "\n",
    "    best_sports = []\n",
    "    best_business = []\n",
    "    best_poltiics= []\n",
    "    best_tech = []\n",
    "    best_entertainment = []\n",
    "\n",
    "\n",
    "    #gets the amount of classes in training set\n",
    "    def amount_of_types(self, np_Array):\n",
    "        for row in np_Array:\n",
    "            if row[2] == \"sport\":\n",
    "                self.total_sport = self.total_sport + 1\n",
    "                self.sports = self.sports + \" \" + row[1]\n",
    "            elif row[2] == \"business\":\n",
    "                self.total_business = self.total_business + 1\n",
    "                self.businesses = self.businesses + \" \" + row[1]\n",
    "            elif row[2] == \"politics\":\n",
    "                self.total_politics = self.total_politics + 1\n",
    "                self.politics = self.politics + \" \" + row[1]\n",
    "            elif row[2] == \"tech\":\n",
    "                self.total_tech = self.total_tech + 1\n",
    "                self.techs = self.techs + \" \" + row[1]\n",
    "            elif row[2] == \"entertainment\":\n",
    "                self.total_entertainment = self.total_entertainment + 1\n",
    "                self.entertainments = self.entertainments + \" \" + row[1]\n",
    "\n",
    "    #creates a bag of words dictionary with using bot unigram and bigram words            \n",
    "    def BagofWords(self, np_Array):\n",
    "        for row in np_Array:\n",
    "            for word in row[1].split():\n",
    "                if word not in ENGLISH_STOP_WORDS:\n",
    "                    if word in self.words_count:\n",
    "                        self.words_count[word] = self.words_count[word] + 1\n",
    "                    else:\n",
    "                        self.words_count[word] = 1\n",
    "                        self.words_dict[word] = word\n",
    "\n",
    "        for row in np_Array:\n",
    "            wordlist = row[1].split(\" \")\n",
    "            for word in range(len(wordlist)-1):\n",
    "                if (wordlist[word] not in ENGLISH_STOP_WORDS) and (wordlist[word+1] not in ENGLISH_STOP_WORDS):\n",
    "                    if (wordlist[word] + \" \" + wordlist[word+1]) in self.words_count:\n",
    "                        self.words_count[wordlist[word] + \" \" + wordlist[word+1]] = self.words_count[wordlist[word] + \" \" + wordlist[word+1]] + 1\n",
    "                    else:\n",
    "                        self.words_count[wordlist[word] + \" \" + wordlist[word+1]] = 1\n",
    "                        self.words_dict[wordlist[word] + \" \" + wordlist[word + 1]] = wordlist[word] + \" \" + wordlist[word + 1]\n",
    "\n",
    "        for key in self.words_count:\n",
    "            self.total = self.words_count[key] + self.total\n",
    "            self.keys.append(key)\n",
    "\n",
    "        self.keys = sorted(self.keys)\n",
    "\n",
    "    #tfidf algorithm to get best words\n",
    "    def tfid(self, training):\n",
    "        corpus = []\n",
    "        corpus.append(self.sports)\n",
    "        corpus.append(self.businesses)\n",
    "        corpus.append(self.politics)\n",
    "        corpus.append(self.techs)\n",
    "        corpus.append(self.entertainments)\n",
    "\n",
    "        cv = CountVectorizer(vocabulary=self.keys, stop_words=ENGLISH_STOP_WORDS)\n",
    "        count_vector = cv.fit_transform(corpus)\n",
    "\n",
    "        tfidf_transformer = TfidfTransformer(smooth_idf=True, use_idf=True)\n",
    "        tfidf_transformer.fit(count_vector)\n",
    "\n",
    "        count_vector = cv.transform(corpus)\n",
    "        tf_idf_vector = tfidf_transformer.transform(count_vector)\n",
    "\n",
    "        self.getTenBestWords(tf_idf_vector.toarray())\n",
    "\n",
    "    #gets top 10 best words\n",
    "    def getTenBestWords(self, tf_idf):\n",
    "\n",
    "        sport = np.argpartition(tf_idf[0], -10)[-10:]\n",
    "        business = np.argpartition(tf_idf[1], -10)[-10:]\n",
    "        politics = np.argpartition(tf_idf[2], -10)[-10:]\n",
    "        tech = np.argpartition(tf_idf[3], -10)[-10:]\n",
    "        entertainment = np.argpartition(tf_idf[4], -10)[-10:]\n",
    "\n",
    "        for i in range(10):\n",
    "            self.best_sports.append(self.words_dict[self.keys[sport[i]]])\n",
    "            self.best_business.append(self.words_dict[self.keys[business[i]]])\n",
    "            self.best_poltiics.append(self.words_dict[self.keys[politics[i]]])\n",
    "            self.best_tech.append(self.words_dict[self.keys[tech[i]]])\n",
    "            self.best_entertainment.append(self.words_dict[self.keys[entertainment[i]]])\n",
    "\n",
    "        print(\"Effective words without stopwords for sport are: \" + str(self.best_sports))\n",
    "        print(\"Effective words without stopwords for business are: \" + str(self.best_business))\n",
    "        print(\"Effective words without stopwords for politics are: \" + str(self.best_poltiics))\n",
    "        print(\"Effective words without stopwords for tech are: \" + str(self.best_tech))\n",
    "        print(\"Effective words without stopwords for entertainment are: \" + str(self.best_entertainment))\n",
    "\n",
    "        \n",
    "#nb classifyer with only using top 10 words without stopwords        \n",
    "class part3_stopwords_classifyer:\n",
    "    corpus = []\n",
    "    keys = []\n",
    "    words_dict = {}\n",
    "    total = 0\n",
    "    guesses = []\n",
    "\n",
    "    total_sport = 0\n",
    "    total_business = 0\n",
    "    total_politics = 0\n",
    "    total_tech = 0\n",
    "    total_entertainment = 0\n",
    "\n",
    "    #gets amount of classes in training set\n",
    "    def amount_of_types(self, np_Array):\n",
    "        for row in np_Array:\n",
    "            if row[2] == \"sport\":\n",
    "                self.total_sport = self.total_sport + 1\n",
    "            elif row[2] == \"business\":\n",
    "                self.total_business = self.total_business + 1\n",
    "            elif row[2] == \"politics\":\n",
    "                self.total_politics = self.total_politics + 1\n",
    "            elif row[2] == \"tech\":\n",
    "                self.total_tech = self.total_tech + 1\n",
    "            elif row[2] == \"entertainment\":\n",
    "                self.total_entertainment = self.total_entertainment + 1\n",
    "\n",
    "    #creates a bag of words dictionary with only top 10 words\n",
    "    def BagofWordsUnigram(self, np_Array, top10):\n",
    "\n",
    "        for i in top10:\n",
    "            if i not in self.words_dict:\n",
    "                self.words_dict[i] = 0\n",
    "\n",
    "        for row in np_Array:\n",
    "            for word in row[1].split():\n",
    "                if word in self.words_dict:\n",
    "                    self.words_dict[word] = self.words_dict[word] + 1\n",
    "\n",
    "            self.corpus.append(row[1])\n",
    "\n",
    "        for key in self.words_dict:\n",
    "            self.total = self.words_dict[key] + self.total\n",
    "            self.keys.append(key)\n",
    "\n",
    "    #vectorizes the training set and test data\n",
    "    def vectorizerUnigram(self, test, training):\n",
    "        vectorizer = CountVectorizer(vocabulary=self.keys)\n",
    "        self.corpus.append(test[1])\n",
    "        X = vectorizer.fit_transform(self.corpus)\n",
    "        vectorizer.get_feature_names_out()\n",
    "        X = X.toarray()\n",
    "\n",
    "        self.corpus.pop()\n",
    "        self.frequenciesUnigram(X, training)\n",
    "\n",
    "    #gets the frequencies of words for each class that the test data has\n",
    "    def frequenciesUnigram(self, X, training):\n",
    "        index_list = []\n",
    "        frequency_list_sport = []\n",
    "        frequency_list_business = []\n",
    "        frequency_list_politics = []\n",
    "        frequency_list_tech = []\n",
    "        frequency_list_entertainment = []\n",
    "        for element in range(len(X[len(X) - 1])):\n",
    "            if X[len(X) - 1][element] > 0:\n",
    "                index_list.append(element)\n",
    "\n",
    "        for i in range(len(index_list)):\n",
    "            frequency_list_sport.append(0)\n",
    "            frequency_list_business.append(0)\n",
    "            frequency_list_politics.append(0)\n",
    "            frequency_list_tech.append(0)\n",
    "            frequency_list_entertainment.append(0)\n",
    "            for element in range(len(X) - 1):\n",
    "                if training[element][2] == \"sport\":\n",
    "                    frequency_list_sport[i] = frequency_list_sport[i] + X[element][index_list[i]]\n",
    "                elif training[element][2] == \"business\":\n",
    "                    frequency_list_business[i] = frequency_list_business[i] + X[element][index_list[i]]\n",
    "                elif training[element][2] == \"politics\":\n",
    "                    frequency_list_politics[i] = frequency_list_politics[i] + X[element][index_list[i]]\n",
    "                elif training[element][2] == \"tech\":\n",
    "                    frequency_list_tech[i] = frequency_list_tech[i] + X[element][index_list[i]]\n",
    "                elif training[element][2] == \"entertainment\":\n",
    "                    frequency_list_entertainment[i] = frequency_list_entertainment[i] + X[element][index_list[i]]\n",
    "\n",
    "        self.nbUnigram(frequency_list_sport, frequency_list_business, frequency_list_politics, frequency_list_tech,\n",
    "                       frequency_list_entertainment, X[len(X) - 1], training, index_list)\n",
    "\n",
    "    #nb classifyer\n",
    "    def nbUnigram(self, fs, fb, fp, ft, fe, test, training, index_list):\n",
    "\n",
    "        nb_s = round(math.log(self.total_sport / len(training)), 8)\n",
    "        nb_b = round(math.log(self.total_business / len(training)), 8)\n",
    "        nb_p = round(math.log(self.total_politics / len(training)), 8)\n",
    "        nb_t = round(math.log(self.total_tech / len(training)), 8)\n",
    "        nb_e = round(math.log(self.total_entertainment / len(training)), 8)\n",
    "\n",
    "        #nb with add one smoothing\n",
    "        if (0 in fs) or (0 in fb) or (0 in fp) or (0 in ft) or (0 in fe):\n",
    "\n",
    "            for i in range(len(fs)):\n",
    "                fs[i] = fs[i] + 1\n",
    "                fb[i] = fb[i] + 1\n",
    "                fp[i] = fp[i] + 1\n",
    "                ft[i] = ft[i] + 1\n",
    "                fe[i] = fe[i] + 1\n",
    "\n",
    "            for i in range(len(fs)):\n",
    "                for j in range(test[index_list[i]]):\n",
    "                    nb_s = nb_s + (round((math.log(fs[i] / ((self.total) + len(self.words_dict)))), 8))\n",
    "                for j in range(test[index_list[i]]):\n",
    "                    nb_b = nb_b + (round((math.log(fb[i] / ((self.total) + len(self.words_dict)))), 8))\n",
    "                for j in range(test[index_list[i]]):\n",
    "                    nb_p = nb_p + (round((math.log(fp[i] / ((self.total) + len(self.words_dict)))), 8))\n",
    "                for j in range(test[index_list[i]]):\n",
    "                    nb_t = nb_t + (round((math.log(ft[i] / ((self.total) + len(self.words_dict)))), 8))\n",
    "                for j in range(test[index_list[i]]):\n",
    "                    nb_e = nb_e + (round((math.log(fe[i] / ((self.total) + len(self.words_dict)))), 8))\n",
    "        \n",
    "        #nb without smoothing\n",
    "        else:\n",
    "            for i in range(len(fs)):\n",
    "                for j in range(test[index_list[i]]):\n",
    "                    nb_s = nb_s + (round((math.log(fs[i] / (self.total))), 8))\n",
    "                for j in range(test[index_list[i]]):\n",
    "                    nb_b = nb_b + (round((math.log(fb[i] / (self.total))), 8))\n",
    "                for j in range(test[index_list[i]]):\n",
    "                    nb_p = nb_p + (round((math.log(fp[i] / (self.total))), 8))\n",
    "                for j in range(test[index_list[i]]):\n",
    "                    nb_t = nb_t + (round((math.log(ft[i] / (self.total))), 8))\n",
    "                for j in range(test[index_list[i]]):\n",
    "                    nb_e = nb_e + (round((math.log(fe[i] / (self.total))), 8))\n",
    "\n",
    "        results = [nb_s, nb_b, nb_p, nb_t, nb_e]\n",
    "\n",
    "        if max(results) == nb_s:\n",
    "            self.guesses.append(\"sport\")\n",
    "        elif max(results) == nb_b:\n",
    "            self.guesses.append(\"business\")\n",
    "        elif max(results) == nb_p:\n",
    "            self.guesses.append(\"politics\")\n",
    "        elif max(results) == nb_t:\n",
    "            self.guesses.append(\"tech\")\n",
    "        elif max(results) == nb_e:\n",
    "            self.guesses.append(\"entertainment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c37941",
   "metadata": {},
   "source": [
    "Here part3 function first creates top10 objects and calculates the most effective words for each subject. Then creates classifier objects and gets accuracies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f7e0843f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#handles part3 operations\n",
    "def part3(training_data, test_data):\n",
    "    \n",
    "    #gets top 10 words for each class\n",
    "    part3_obj = part3_top10()\n",
    "    part3_obj.amount_of_types(training_data)\n",
    "    part3_obj.BagofWords(training_data)\n",
    "    part3_obj.tfid(training_data)\n",
    "\n",
    "    #gets top 10 words for each class without stopwords\n",
    "    part3_stopwords_obj = part3_stopwords()\n",
    "    part3_stopwords_obj.amount_of_types(training_data)\n",
    "    part3_stopwords_obj.BagofWords(training_data)\n",
    "    part3_stopwords_obj.tfid(training_data)\n",
    "\n",
    "    #classifies using top 10 words\n",
    "    part3_classifyer_obj = part3_classifyer()\n",
    "    part3_classifyer_obj.amount_of_types(training_data)\n",
    "    top10 = part3_obj.best_sports + part3_obj.best_business + part3_obj.best_poltiics + part3_obj.best_tech + part3_obj.best_entertainment\n",
    "    part3_classifyer_obj.BagofWords(training_data, top10)\n",
    "    for test in test_data:\n",
    "        part3_classifyer_obj.vectorizer(test, training_data)\n",
    "    \n",
    "    #classifies using top 10 words without stopwords\n",
    "    part3_stopwords_classifyer_obj = part3_stopwords_classifyer()\n",
    "    part3_stopwords_classifyer_obj.amount_of_types(training_data)\n",
    "    top10 = part3_stopwords_obj.best_sports + part3_stopwords_obj.best_business + part3_stopwords_obj.best_poltiics + part3_stopwords_obj.best_tech + part3_stopwords_obj.best_entertainment\n",
    "    part3_stopwords_classifyer_obj.BagofWordsUnigram(training_data, top10)\n",
    "    for test in test_data:\n",
    "        part3_stopwords_classifyer_obj.vectorizerUnigram(test, training_data)\n",
    "\n",
    "    #gets accuracy for top10 words classification\n",
    "    count = 0\n",
    "\n",
    "    for i in range(len(test_data)):\n",
    "        if test_data[i][2] == part3_classifyer_obj.guesses[i]:\n",
    "            count = count + 1\n",
    "        else:\n",
    "            pass\n",
    "    print(\"Accuracy for guesses with effective words: \" + str(count / 298))\n",
    "\n",
    "    #gets accuracy for top10 words classification\n",
    "    count2 = 0\n",
    "\n",
    "    for i in range(len(test_data)):\n",
    "        if test_data[i][2] == part3_stopwords_classifyer_obj.guesses[i]:\n",
    "            count2 = count2 + 1\n",
    "        else:\n",
    "            pass\n",
    "    print(\"Accuracy for guesses with effective words without stop words: \" + str(count2 / 298))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d08240",
   "metadata": {},
   "source": [
    "Here we can see the most effective words for each subject with and without stopwords. As we can if we dont eliminate stop words all we get are stop words which results in accuracy being as low as %26. When we eliminate stopwords  we get more logical results with words being actually about these subjects and accuracy going up to %83.\n",
    "\n",
    "We not getting any bigram words for this job could be the same reason why we getting stopwords if we dont eliminate them. As stopwords appear more often in each category we get them as a result. And as the bigram words appear less often in each category we don't get them.\n",
    "\n",
    "Accuracy being lower than using all words could have 2 reasons. One being the using ten words could be not suitable for our dataset maybe the sweet spot is higher than ten words. Other reason could be that in our general stopwords word \"said\" and \"mr\" is not included but they also appear to act like stopwords for our dataset. Adding those to the stopwords could increase the accuracy here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "17372a65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Effective words for sport are: ['is', 'it', 'on', 'the', 'in', 'to', 'he', 'of', 'for', 'and']\n",
      "Effective words for business are: ['it', 'that', 'for', 'is', 'and', 'in', 'said', 'of', 'the', 'to']\n",
      "Effective words for politics are: ['on', 'that', 'for', 'he', 'said', 'and', 'to', 'of', 'in', 'the']\n",
      "Effective words for tech are: ['on', 'for', 'it', 'is', 'to', 'that', 'of', 'the', 'and', 'in']\n",
      "Effective words for entertainment are: ['film', 'it', 'on', 'was', 'for', 'to', 'of', 'the', 'and', 'in']\n",
      "Effective words without stopwords for sport are: ['world', 'coach', 'cup', 'win', 'chelsea', 'time', 'game', 'said', 'england', 'year']\n",
      "Effective words without stopwords for business are: ['economy', 'company', 'sales', 'yukos', 'new', 'growth', 'mr', 'said', 'year', 'market']\n",
      "Effective words without stopwords for politics are: ['tories', 'brown', 'blair', 'party', 'people', 'government', 'said', 'labour', 'mr', 'election']\n",
      "Effective words without stopwords for tech are: ['technology', 'digital', 'new', 'mr', 'microsoft', 'mobile', 'people', 'software', 'users', 'said']\n",
      "Effective words without stopwords for entertainment are: ['chart', 'actress', 'festival', 'new', 'album', 'music', 'year', 'film', 'best', 'said']\n",
      "Accuracy for guesses with effective words: 0.2550335570469799\n",
      "Accuracy for guesses with effective words without stop words: 0.825503355704698\n"
     ]
    }
   ],
   "source": [
    "part3(training_data, test_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
